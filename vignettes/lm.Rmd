---
title: "Estimating Linear Models with the rstanarm Package"
author: "Jonah Gabry and Ben Goodrich"
date: "08/30/2015"
output: 
  html_document: 
    fig_caption: yes
    toc: yes
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Estimating Linear Models with the rstanarm Package}
-->
```{r, child="SETTINGS-knitr.txt"}
```
```{r, child="SETTINGS-gg.txt"}
```
```{r, child="SETTINGS-rstan.txt"}
```

# Introduction

This vignette explains how to estimate linear models using the
__rstanarm__ package.

```{r, child = "four_steps.txt"}
```

Steps 3 and 4 are covered in more depth by the vignette entitled "How to Use the
__rstanarm__ Package". This vignette focuses on Step 1 when the likelihood is
the product of independent normal distributions.

The goal of the __rstanarm__ package is to make Bayesian estimation of common
regression models routine. That goal can be partially accomplished by providing
interfaces that are similar to the popular formula-based interfaces to 
frequentist estimators of those regression models. But fully accomplishing that
goal sometimes entails utilizing priors that applied researchers are unaware
that they prefer. These priors are intended to work well for any data that a
user might pass to the interface that was generated according to the assumptions
of the likelihood function.

It is important to distinguish between priors that are easy for applied 
researchers to _specify_ and priors that are easy for applied researchers to 
_conceptualize_. The prior described below emphasizes the former but we outline
its derivation so that applied researchers may feel more comfortable utilizing 
it.

# Likelihood

The likelihood for one observation under a linear model can be written as a
conditionally normal PDF
$$\frac{1}{\sigma_{\epsilon} \sqrt{2 \pi}} 
  e^{-\frac{1}{2} \left(\frac{y - \mu}{\sigma_{\epsilon}}\right)^2},$$
where $\mu = \alpha + \mathbf{x}^\top \boldsymbol{\beta}$ is a linear predictor
and $\sigma_{\epsilon}$ is the standard deviation of the error in predicting
the outcome, $y$. The likelihood of the entire sample is the product of $N$
individual likelihood contributions.

It is well-known that the likelihood of the sample is maximized when
$$ \widehat{\boldsymbol{\beta}} = \left(\mathbf{X} \mathbf{X}^\top\right)^{-1}
                                   \mathbf{X}^\top \mathbf{y}, $$
$$ \widehat{\alpha} = \overline{y} - \overline{\mathbf{x}}^\top 
                                     \widehat{\boldsymbol{\beta}}, $$
$$ \widehat{\sigma}_{\epsilon}^2 = 
  \frac{\left(\mathbf{y} - \widehat{\alpha} - \mathbf{X} \widehat{
                                              \boldsymbol{\beta}}\right)^\top
        \left(\mathbf{y} - \widehat{\alpha} - \mathbf{X} \widehat{
                                              \boldsymbol{\beta}}\right)}{N},$$
where $\overline{\mathbf{x}}$ is a vector that contains the sample means of the
$K$ predictors, $\mathbf{X}$ is a $N \times K$ matrix of centered predictors, 
$\mathbf{y}$ is a $N$-vector of outcomes and $\overline{y}$ is the sample mean 
of the outcome.

# Priors

The key innovation in the `stan_lm` function in the __rstanarm__ package is the
implied prior for $\boldsymbol{\beta}$ and $\sigma_{\epsilon}$. To understand
this prior, think about the equations that characterize the maximum likelihood
solutions _stochastically_, which is to say prior to observing the data on 
$\mathbf{X}$ and $\mathbf{y}$. What would the distribution of 
$\boldsymbol{\beta}$ and $\sigma_{\epsilon}$ look like under an assumed 
_distribution_ of the data?

We start by noting that $\beta_k = \frac{\sigma_y}{\sigma_{x_k}} \lambda_k$ 
where $\lambda_k$ is the $k$-th _standardized_ regression coefficient, whose
MLE is given by $\widehat{\boldsymbol{\lambda}} = \mathrm{Cor\left(\mathbf{X},
\mathbf{X}\right)}^{-1} \mathrm{Cor\left(\mathbf{X},\mathbf{y}\right)}$. Thus,
a prior for $\boldsymbol{\lambda}$ --- along with a prior for $\sigma_y$ and
an _estimate_ of each $\sigma_{x_k}$ --- implies a prior for each $\beta_k$.

We set $\sigma_y = \omega \widehat{\sigma}_y$ where $\widehat{\sigma}_y$ is the
sample standard deviation of the outcome and $\omega > 0$ is an unknown scale 
parameter to be estimated. The only prior for $\omega$ that does not contravene
Bayes' theorem in this situation is Jeffreys prior, $f\left(\omega\right) 
\propto \frac{1}{\omega}$, which is proportional to a Jeffreys prior on the 
unkown $\sigma_y$, $f\left(\sigma_y\right) \propto \frac{1}{\sigma_y} = 
\frac{1}{\omega \widehat{\sigma}_y} \propto \frac{1}{\omega}$.

To complete the prior for $\boldsymbol{\lambda}$ we need a joint prior for 
$\mathrm{Cor\left(\mathbf{X},\mathbf{X}\right)}$ and 
$\mathrm{Cor\left(\mathbf{X},\mathbf{y}\right)}$. Let the prior correlation
matrix among the $K$ predictors and the outcome be
$$\boldsymbol{\Lambda}=\left[\begin{array}{c|c}
\overbrace{\boldsymbol{\boldsymbol{\Lambda}}_{XX}}^{K\times K} & \overbrace{\boldsymbol{\Lambda}_{XY}}^{K\times1}\\
\hline \underbrace{\boldsymbol{\boldsymbol{\Lambda}}_{XY}^{\top}}_{1\times K} & 
\underbrace{1}_{1\times1}
\end{array}\right]$$
Thus, a prior distribution for $\boldsymbol{\Lambda}$ implies a prior 
distribution for $\boldsymbol{\lambda} = \left(\boldsymbol{\Lambda}_{XX}\right)
^{-1} \boldsymbol{\Lambda}_{XY}$.

The Stan project has popularized a prior distribution for correlation matrices
derived by Lewandowski, Kurowicka, and Joe in their seminal $2009$ paper 
"Generating random correlation matrices based on vines and extended onion 
method", published in the _Journal of Multivariate Analysis_, volume $100$, 
pages $1989$ --- $2001$. This "LKJ" PDF is characterized by
$$f\left(\boldsymbol{\Lambda}|\eta\right) \propto 
\left(\mathrm{det{\boldsymbol{\Lambda}}}\right)^{\eta - 1},$$
where $\mathrm{det}\left(\cdot\right)$ is the determinant function and 
$\eta > 0$ is a shape hyperparameter. The expectation of $\boldsymbol{\Lambda}$
is the identity matrix of order $K + 1$.

Since $\boldsymbol{\lambda} = \frac{1}{\mathrm{det} \boldsymbol{\Lambda}_{XX}}
\mathbf{A} \boldsymbol{\Lambda}_{XY}$, where $\mathbf{A}$ is the adjugate 
matrix (i.e. the transpose of the matrix of cofactors of 
$\boldsymbol{\Lambda}_{XX}$), the prior distribution of $\boldsymbol{\lambda}$
has long tails because $\mathrm{det} \boldsymbol{\Lambda}_{XX}$ can be 
arbitrarily close to zero. Also, since the expectation of 
$\boldsymbol{\Lambda}_{XX}$ is the identity matrix of order $K$ and the
expectation of $\boldsymbol{\Lambda}_{XY}$ is a zero vector of order $K$, the
Law of Iterated Expectations implies that $\mathbb{E} \boldsymbol{\lambda} = 
\mathbf{0}$. Thus, the many attempts in the literature to derive a long-tailed,
symmetric, mean-zero prior distribution for (usually standardized) regression 
coefficients can be seen as an approximation to the prior for 
$\boldsymbol{\lambda}$ derived here.

Moreover, since $R^2 = \boldsymbol{\lambda}^\top 
\mathrm{Cor\left(\mathbf{X},\mathbf{X}\right)} \boldsymbol{\lambda}$ is the
proporion of variance in the outcome attributable to the predictors in a linear
model, we can substitute the unknown $\boldsymbol{\Lambda}_{XX}$ for the 
estimated $\mathrm{Cor\left(\mathbf{X},\mathbf{X}\right)}$ and consider the 
implied prior distribution on $R^2$. It turns out that the LKJ prior on
$\boldsymbol{\Lambda}$ implies a $\mathrm{Beta}\left(\frac{K}{2},\eta\right)$ 
prior for $R^2$. Thus, any prior information about the location of $R^2$ under
this $\mathrm{Beta}\left(\frac{K}{2},\eta\right)$ prior can be used to choose a
value of the hyperparameter $\eta$. The `R2(location, what)` function in the 
__rstanarm__ package supports four ways of choosing $\eta$:

1. `what = "mode"` and `location` is some prior mode on the $\left(0,1\right)$
  interval. This is the default but since the mode of a 
  $\mathrm{Beta}\left(\frac{K}{2},\eta\right)$ distribution is
  $\frac{\frac{K}{2} - 1}{\frac{K}{2} + \eta - 2}$ the mode only exists if
  $K > 2$. If $K \leq 2$, then the user must specify something else for `what`.
2. `what = "mean"` and `location` is some prior mean on the $\left(0,1\right)$
  interval, where the mean of a $\mathrm{Beta}\left(\frac{K}{2},\eta\right)$ 
  distribution is $\frac{\frac{K}{2}}{\frac{K}{2} + \eta}$.
3. `what = "median"` and `location` is some prior median on the 
  $\left(0,1\right)$ interval. The median of a 
  $\mathrm{Beta}\left(\frac{K}{2},\eta\right)$ distribution is not available
  in closed form but if $K > 2$ is approximately equal to 
  $\frac{\frac{K}{2} - \frac{1}{3}}{\frac{K}{2} + \eta - \frac{2}{3}}$. 
  Regardless of whether $K > 2$, the `R2` function can numerically solve for 
  the value of $\eta$ that is consistent with a given prior median utilizing
  the quantile function.
4. `what = "log"` and `location` is some (negative) prior value for 
  $\mathbb{E} \ln R^2 = \psi\left(\frac{K}{2}\right)-
  \psi\left(\frac{K}{2}+\eta\right)$, where $\psi\left(\cdot\right)$ is the
  `digamma` function. Again, given a prior value for the left-hand side it
  is easy to numerically solve for the corresponding value of $\eta$.
  
There is no default value for the `location` argument of the `R2` function.
This is an _informative_ prior on $R^2$, which must be chosen by the user
in light of the research project. However, specifying `location = 0.5` is
often safe, in which case $\eta = \frac{K}{2}$ regardless of whether `what`
is `"mode"`, `"mean"`, or `"median"`.

It would seem that we need a prior for $\sigma_{\epsilon}$, but our prior
beliefs about $\sigma_{\epsilon} = \omega \widehat{\sigma}_y \sqrt{1 - R^2}$ 
are already implied by our prior beliefs about $\omega$ and $R^2$. That
only leaves a prior for $\alpha = \overline{y} - \overline{\mathbf{x}}^\top
\boldsymbol{\beta}$. The obvious choice is a normal prior with expectation
$\overline{\mathbf{x}}^\top \boldsymbol{\beta}$ but what standard deviation?
We utilize the formula for the standard _error_ of the maximum likelihood
estimate of $\alpha$, but in order to make it a prior standard deviation, 
substitute the unknown $\boldsymbol{\Lambda}_{XX}$ for the estimated
$\mathrm{Cor\left(\mathbf{X},\mathbf{X}\right)}$.

# Posterior

The previous sections imply that the posterior distribution is
$$f\left(\omega, \boldsymbol{\Lambda}, \alpha | \mathbf{y}, \mathbf{X}, \eta
\right) \propto \frac{1}{\omega} \left(\mathrm{det{\boldsymbol{\Lambda}}}\right
)^{\eta - 1} f\left(\alpha | \boldsymbol{\Lambda}\right) \prod_{i=1}^N{\frac{1}
{\sigma_{\epsilon}} e^{-\frac{1}{2} \left(\frac{y_i - \mu_i}{\sigma_{\epsilon}}
\right)^2}},$$
where $f\left(\alpha | \boldsymbol{\Lambda}\right)$ is the normal prior 
described above and the posterior distributions of the intermediate parameters 
are all implied by the posterior distributions of $\boldsymbol{\Lambda}$ and 
$\omega$:

* $\boldsymbol{\lambda} = \left(\boldsymbol{\Lambda}_{XX}\right)^{-1} 
  \boldsymbol{\Lambda}_{XY}$
* $R^2 = \boldsymbol{\lambda}^\top \boldsymbol{\Lambda}_{XX} 
  \boldsymbol{\lambda}$
* $\sigma_y = \omega \widehat{\sigma}_y$
* $\sigma_{\epsilon} = \sigma_y \sqrt{1 - R^2}$
* $\beta_k = \frac{\sigma_y}{\widehat{\sigma}_{x_k}} \lambda_k \forall k$
* $\mu_i = \alpha + \overline{\mathbf{x}}^\top \boldsymbol{\beta}

The implementation actually utilizes $\ln \omega$. Consequently, if
$\ln \omega = 0$, then the marginal standard deviation of the outcome
_implied by the model_ is the same as the sample standard deviation of the
outcome. If $\ln \omega > 0$, then the marginal standard deviation of the 
outcome implied by the model exceeds the sample standard deviation, implying
that the model overfits the data. If $\ln \omega < 0$, then the marginal 
standard deviation of the outcome implied by the model is less than the 
sample standard deviation, implying that the model _underfits_ the data.
However, given the regularizing nature of the prior on $\boldsymbol{\lambda}$,
a minor underfit would be considered ideal if the goal is to obtain good
out-of-sample predictions.
  
If the model badly underfits or overfits the data, then you may want to
reconsider the model. However, for small $N$, it is often the case that the 
marginal posterior distributions of $\sigma_{\epsilon}$, $\alpha$ and
$\boldsymbol{\beta}$ are fairly reasonable even if the posterior 
distributions of $\ln \omega$ and $\boldsymbol{\Lambda}$ are not. This 
counterintuitive result is due to the fact that the data contain information 
about the parameters that actually enter the likelihood function --- 
$\sigma_{\epsilon}$, $\alpha$, and $\boldsymbol{\beta}$ --- but any given 
distribution of these three parameters may be consistent with a multimodal 
distribution of the hyperparameters $\ln \omega$ and $\boldsymbol{\Lambda}$.

# Example

We will utilize an example from the __HSAUR3__ package by Brian S. Everitt and
Torsten Hothorn, which is used in their 2014 book 
_A Handbook of Statistical Analyses Using R (3rd Edition)_ (Chapman & Hall /
CRC). This book is frequentist in nature and we will show how to obtain the 
corresponding Bayesian results.

The model in section 5.3.1 analyzes an experiment where clouds were seeded
with different amounts of silver iodide to see if there was increased rainfall.
This effect could vary according to covariates, which (except for `time`) are
interacted with the treatment variable. Most people would probably be skeptical
that cloud hacking could explain very much of the variation in rainfall and
thus the prior mode of the $R^2$ would probably be fairly small.

The frequentist estimator of this model can be replicated by executing
```{r OLS}
data("clouds", package = "HSAUR3")
ols <- lm(rainfall ~ seeding * (sne + cloudcover + prewetness + echomotion) +
            time, data = clouds)
round(coef(ols), 3)
```
Note that we have _not_ looked at the estimated $R^2$ or $\sigma$ for the 
ordinary least squares model. We can estimate a Bayesian version of this model
by prepending `stan_` to the `lm` call, specifying a prior mode for $R^2$, and
optionally specifying how many cores the computer may utilize:
```{r MCMC}
library(rstanarm)
post <- stan_lm(rainfall ~ seeding * (sne + cloudcover + prewetness + 
                                        echomotion) + time, data = clouds,
                prior = R2(location = 0.25), 
                chains = CHAINS, cores = CORES, seed = SEED)
post
```
In this case, the "Bayesian point estimates", which are represented by the 
posterior medians, appear quite different from the ordinary least squares
estimates. However, the `log-fit_ratio` (i.e. $\ln \omega$) is quite small,
indicating that the model only slightly overfits the data when the prior
derived above is utilized. Thus, it would be safe to conclude that the ordinary
least squares estimator considerably overfits the data since there are only 
$24$ observations to estimate $12$ parameters with.

Also, it is not obvious what the estimated average treatment effect is since
the treatment variable, `seeding`, is interacted with four other correlated
predictors. However, it is easy to estimate or visualize the average treatment
effect using the `posterior_predict` function in the __rstanarm__ package.
```{r ATE,fig.width=9}
clouds_cf <- clouds
clouds_cf$seeding[] <- "yes"
y1_rep <- posterior_predict(post, newdata = clouds_cf)
clouds_cf$seeding[] <- "no"
y0_rep <- posterior_predict(post, newdata = clouds_cf)
par(mar = c(5,4,0,0) + .1, las = 1)
hist(y1_rep - y0_rep, prob = TRUE, main = "", xlab = "Estimated ATE")
```

As can be seen, the treatment effect is not estimated precisely and is as
likely to be negative as it is to be positive.

# Alternative Approach

The prior derived above works well in many situations and is quite simple
to _use_ since it only requires the user to specify the prior location of
the $R^2$. Nevertheless, the implications of the prior are somewhat difficult 
to _conceptualize_. Thus, it is perhaps worthwhile to compare to another 
estimator of a linear model that simply puts independent Cauchy priors on the 
regression coefficients. This simpler approach can be executed by calling the 
`stan_glm` function with `family = gaussian()` and specifying the priors:
```{r SIMPLE}
simple <- stan_glm(rainfall ~ seeding * (sne + cloudcover + prewetness + 
                                        echomotion) + time, data = clouds,
                   prior = cauchy(), prior_intercept = cauchy(),
                   chains = CHAINS, cores = CORES, seed = SEED)
```
We can compare the two approaches using an approximation to Leave-One-Out
(LOO) cross-validation, which is implemented by the `loo` function in the
__loo__ package.
```{r LOO}
(loo_post <- loo(post, cores = 2))
loo(simple, cores = 2)
```
The results are so similar that we have only a small basis to prefer the 
simpler approach, but the Warning messages are important here. Most of the
estimated shape parameters for the Generalized Pareto distribution are above
$0.5$ in the model with simpler priors, which indicates that these estimates
are only going to converge slowly to the true out-of-sample deviance 
measures. Thus, with only $24$ observations, they should not be considered 
reliable. The more complicated prior derived above is stronger --- as 
evidenced by the fact that the effective number of parameters is only $6.4$
as compared to $11.3$ in the simpler approach and $12$ for a frequentist 
estimator --- and only has $3$ out of $24$ shape estimates in the "danger 
zone". We might want to reexamine these three observations (1, 2, and 15)
```{r LOO_POST}
plot(loo_post, label_points = TRUE)
```

because the posterior is sensitive to them but, overall, the results seem 
tolerable.

In general, we would expect the joint prior derived here to work better 
when there are many predictors relative to the number of observations.
Placing independent, heavy-tailed priors on the coefficients neither
reflects the beliefs of the researcher nor conveys enough information to
stabilize all the computations.

# Conclusion

This vignette has discussed the prior distribution utilized in the `stan_lm`
function, which is otherwise similar to the `lm` function in R. Although
this prior has desirable characteristics, none of them are obvious, which
makes it difficult for applied researchers to _conceptualize_ the prior.
Nevertheless, anyone should be able to _specify_ the prior because it only
requires the researcher to specify the prior location of the $R^2$, which
is the familiar proportion of variance in the outcome variable that is
attributable to the predictors under a linear model. Anyone who does not
have a plausible guess for $R^2$ has not thought about the data-generating
process hard enough.

While the approach outlined here only requires the user to specify a scalar
bounded on the $\left(0,1\right)$ interval, other Bayesian approaches to
prior specification in a linear regression model require the researcher to
specify a prior distribution for each regression coefficient (and the 
intercept and error variance). Most researchers have little capability to
specify all these prior distributions thoughtfully and take a short-cut
by specifying one prior distribution that is taken to apply to all the
regression coefficients as if they were independent of each other (and the 
intercept and error variance). This short-cut is available in the `stan_glm` 
function and is described in more detail in other __rstanarm__ vignettes for
Generalized Linear Models (GLMs).

However, this short-cut should be resisted because while it is much easier
to assert a prior distribution applies independently to many regression
coefficients, no one really believes _a priori_ that regression coefficients
are independent of each other (which would entail that the predictors are
uncorrelated with each other). Specifying a joint prior distribution over
the parameters in a linear regression model is difficult, as the previous
sections attest, but it is possible and can be automated given some prior
information about the location of the $R^2$. 

Thus, we are optimistic that this prior will greatly help in accomplishing
our goal for __rstanarm__ of making Bayesian estimation of regression models
routine. The same approach is used to specify a prior in ANOVA models (see
`stan_aov`) and proportional-odds models for ordinal outcomes (see 
`stan_polr`).
