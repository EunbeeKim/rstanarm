---
title: "Estimating Generalized Linear Models for Count Data with the rstanarm Package"
author: "Jonah Gabry and Ben Goodrich"
date: "09/03/2015"
output: 
  html_document: 
    fig_caption: yes
    toc: yes
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Estimating Generalized Linear Models for Binary Data with the rstanarm Package}
-->
```{r, HOOK, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    x = paste(x, collapse = '.')
    if (!grepl('\\.svg', x)) return(hook_plot(x, options))
    # read the content of the svg image and write it out without <?xml ... ?>
    paste(readLines(x)[-1], collapse = '\n')
  })
})
```

```{r, SETTINGS, include=FALSE}
opts_chunk$set(dev = 'svg', fig.align='center', fig.height = 3, 
               message = FALSE, warning = FALSE) 
suppressPackageStartupMessages(require(rstanarm))
thm_els <- theme(axis.text.y = element_blank(), legend.position = "none",
                 legend.background = element_rect(fill = "gray"))
theme_set(theme_classic() %+replace% thm_els)
```

# Introduction

This vignette explains how to estimate generalized linear models (GLMs) for 
count data using the __rstanarm__ package.

The four steps of a Bayesian analysis are

1. Specify a joint distribution for the outcome(s) and all the unknowns, which
  typically takes the form of a marginal prior distribution for the unknowns
  multiplied by a likelihood for the outcome(s) conditional on the unknowns.
  This joint distribution is proportional to a posterior distribution of the
  unknowns conditional on the observed data
2. Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).
3. Evaluate how well the model fits the data and possibly revise the model.
4. Draw from the posterior predictive distribution of the outcome(s) given
  interesting values of the predictors in order to visualize how a manipulation
  of a predictor affects (a function of) the outcome(s).
  
Steps 3 and 4 are covered in more depth by the vignette entitled "How to Use the
__rstanarm__ Package". This vignette focuses on Step 1 for Poisson and negative
binomial regression models.

# Likelihood

If the outcome for a single observation $y$ is assumed to follow a Poisson distribution,
the likelihood for one observation can be written as a conditionally Poisson PMF

$$\tfrac{1}{y!} \lambda^y e^{-\lambda},$$

where $\lambda = E(y | \mathbf{x}) = g^{-1}(\eta)$ and $\eta = \alpha +
\mathbf{x}^\top \boldsymbol{\beta}$ is a linear predictor. For the Poisson 
distribution it is also true that $\lambda = Var(y | \mathbf{x})$, i.e. the 
mean and variance are both $\lambda$. Later in this vignette we also show how
to estimate a negative binomial regression, which relaxes this assumption of 
equal conditional mean and variance of $y$. 

Because the rate parameter $\lambda$ must be positive, for a Poisson GLM the
_link_ function $g$ maps between the positive real numbers $\mathbb{R}^+$ (the
support of $\lambda$) and the set of all real numbers $\mathbb{R}$. When applied
to a linear predictor $\eta$ with values in $\mathbb{R}$, the inverse link
function $g^{-1}(\eta)$ therefore returns a positve real number.

Although other link functions are possible, the canonical link function for a 
Poisson GLM is the log link $g(x) = \ln{(x)}$. With the log link, the inverse
link function is simply the exponential function and the likelihood for a single
observation becomes

$$\frac{g^{-1}(\eta)^y}{y!} e^{-g^{-1}(\eta)} = 
\frac{e^{\eta y}}{y!} e^{-e^\eta}.$$

# Priors

A full Bayesian analysis requires specifying prior distributions $f(\alpha)$ and
$f(\boldsymbol{\beta})$ for the intercept and vector of regression coefficients.
When using `stan_glm`, these distributions can be set using the 
`prior_intercept` and `prior` arguments. The `stan_glm` function supports a
variety of prior distributions, which are explained in the __rstanarm__
documentation (`help(priors, package = 'rstanarm')`).

As an example, suppose we have $K$ predictors and believe --- prior to seeing 
the data --- that $\alpha, \beta_1, \dots, \beta_K$ are as likely to be positive
as they are to be negative, but are highly unlikely to be far from zero. These
beliefs can be represented by normal distributions with mean zero and a small
scale (standard deviation). To give $\alpha$ and each of the $\beta$s this prior
(with a scale of 1, say), in the call to `stan_glm` we would include the
arguments `prior_intercept = normal(0,1)` and 
`prior = normal(0,1)`.

If, on the other hand, we have less a priori confidence that the parameters will
be close to zero then we could use a larger scale for the normal distribution 
and/or a distribution with heavier tails than the normal like the Student t 
distribution. __Step 1__ in the "How to Use the __rstanarm__ Package" vignette 
discusses one such example.

# Posterior

With independent prior distributions, the joint posterior distribution for
$\alpha$ and $\boldsymbol{\beta}$ is proportional to the product of the priors 
and the $N$ likelihood contributions:

$$f\left(\alpha,\boldsymbol{\beta} | \mathbf{y},\mathbf{X}\right) \propto
  f\left(\alpha\right) \times \prod_{k=1}^K f\left(\beta_k\right) \times
  \prod_{i=1}^N {
  \frac{g^{-1}(\eta_i)^{y_i}}{y_i!} e^{-g^{-1}(\eta_i)}}.$$
  
This is posterior distribution that `stan_glm` will draw from when using MCMC.

# Poisson Regression Example

This example comes from Chapter 8.3 of 
[Gelman and Hill (2007)](http://www.stat.columbia.edu/~gelman/arm/). 

We want to make inferences about the efficacy of a certain pest 
management system at reducing the number of roaches in urban apartments. Here is
how Gelman and Hill describe the experiment (pg. 161):

> [...] the treatment and control were applied to 160 and 104 apartments, 
respectively, and the outcome measurement $y_i$ in each apartment $i$ was the 
number of roaches caught in a set of traps. Different apartments had traps for 
different numbers of days [...] 

In addition to an intercept, the regression predictors for the model are the 
pre-treatment number of roaches `roach1`, the treatment indicator 
`treatment`, and a variable indicating whether the apartment is in a building
restricted to elderly residents `senior`. Because the number of days for which
the roach traps were used is not the same for all apartments in the sample, we
include it as an exposure, which slightly changes the model described in
the __Likelihood__ section above in that the rate parameter $\lambda_i =
exp(\eta_i)$ is multiplied by the exposure $u_i$ giving us 
$y_i \sim Poisson(u_i \lambda_i)$. This is equivalent to adding $\ln{(u_i)}$ 
to the linear predictor $\eta_i$ and it can be specified using the `offset` 
argument to `stan_glm`:

```{r, ROACHES-fit}
# Load data and create dist100 variable 
roaches <- read.csv("roaches.csv.xz")
# Estimate original model
glm1 <- glm(y ~ roach1 + treatment + senior, data = roaches,
            family = poisson, offset = log(exposure2))
# Estimate Bayesian version with stan_glm
stan_glm1 <- stan_glm(y ~ roach1 + treatment + senior, data = roaches, 
                 family = poisson, offset = log(exposure2), 
                 prior = normal(0,2.5), prior_intercept = normal(0,5),
                 cores = 2, seed = 12345)
```

The `formula`, `data`, `family`, and `offset` arguments to `stan_glm` can be 
specified in exactly the same way as for `glm`. The `poisson` family function 
defaults to using the log link, but to write code readable to someone not 
familiar with the defaults we should be explicit and use 
`family = poisson(link = "log")`.  

We've also specified some optional arguments. The `cores` parameter controls the
number of cores utilized by the computer when fitting the model. We also 
provided a `seed` so that we have the option to deterministically reproduce
these results at any time. The `stan_glm` function has many other optional 
arguments that allow for more user control over the way estimation is performed.
The help page for `stan_glm` has more information about these controls as well
as other topics related to GLM estimation (`?stan_glm`, `help(stan_glm, package = 'rstanarm')`).

Here are the point estimates and uncertainties from the `glm` fit and `stan_glm`
fit, which we see are nearly identical:

```{r, ROACHES-glm-ests}
round(rbind(glm = coef(glm1), stan_glm = coef(stan_glm1)), digits = 2)
rbind(glm = summary(glm1)$coefficients[, "Std. Error"], stan_glm = se(stan_glm1))
```

(Note: the dataset we have is slightly different from the one used in Gelman and
Hill (2007), which leads to slightly different parameter estimates than those
shown in the book even when copying the `glm` call verbatim. For the purposes of
this example, the actual estimates are less important than the process.)

Gelman and Hill next show how to compare the observed data to replicated 
datasets from the model to check the quality of the fit. Here we don't show the
original code used by Gelman and Hill because it's many lines, requiring several
loops and some care to get the matrix multiplications right (see pg. 161-162).
On the other hand, the __rstanarm__ package makes this trivial. We can generate
replicated datasets with a single line of code using the `posterior_predict`
function:

```{r, ROACHES-posterior_predict}
yrep <- posterior_predict(stan_glm1)
```

(For more on `posterior_predict` see the "How to Use the __rstanarm__ Package"
vignette.)

Gelman and Hill take the simulated datasets and for each of them  
compute the proportion of zeros and compare to the observed proportion of 
in the original data. We can do the same thing with our replicated 
datasets in `yrep` like this:

```{r, ROACHES-posterior_predict2}
Test <- function(y) mean(y == 0)
T_y <- Test(roaches$y) # value for observed data
T_yrep <- apply(yrep, 1, Test) # value in each of the replicated data sets
print(range(T_yrep), digits = 1)
print(T_y)
```

About 36% of the original observations are zeros, whereas even the replicated 
dataset with the most zeros has only 0.8%. This is a sign that we should 
consider a model that more accurately accounts for the large proportion of zeros
in the data. Gelman and Hill show how we can do this using an overdispered
Poisson regression. To illustrate the use of a different `stan_glm` model, here
we will instead try 
[negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) 
regression, which is also used for
overdispersed or zero-inflated count data. The negative binomial distribution
allows the (conditional) mean and variance of $y$ to differ unlike the Poisson
distribution. We just need to change the `family` we specify in the call
to `stan_glm`, which we now set to `neg_binomial_2` instead of `poisson`:

```{r, ROACHES-negbin, eval=FALSE}
stan_glm2 <- update(stan_glm1, family = neg_binomial_2) 
```

We can also use `posterior_predict` in the same way to check the proportion
of zeros in the replicated datasets:

```{r, ROACHES-negbin-post_pred, eval=FALSE}
yrep2 <- posterior_predict(stan_glm2) 
T_yrep2 <- apply(yrep2, 1, Test)
print(range(T_yrep2), digits = 1)
```

We can see that the replicated datasets under the negative binomial model have
many more zeros that the the replicatations generated under the original Poisson
model. We can more clearly compare the new replications to the original data
with a simple histogram that, in the case, shows that the observed data
is quite plausible under the model:

```{r, ROACHES-negbin2, eval=FALSE}
qplot(T_yrep2, ylab = "", xlab = "Proportion of zeros") + 
  geom_vline(xintercept = T_y, color = "skyblue2", size = 2) + 
  ggtitle("Distribution of the proportion of zeros in replicated data under
          the negative binomial model. Observed proportion is the blue line.")
```

